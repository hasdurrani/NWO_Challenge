{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import networkx as nx\n",
    "from ordered_set import OrderedSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Query\n",
    "\n",
    "- Reddit Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/google/cloud/bigquery/client.py:444: UserWarning: Cannot create BigQuery Storage client, the dependency google-cloud-bigquery-storage is not installed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 8)\n"
     ]
    }
   ],
   "source": [
    "def make_query_reddit(query_term, limit):\n",
    "    \"\"\"Function to query the reddit dataset for a particular \n",
    "    term and return the data as a pandas dataframe\"\"\"\n",
    "    \n",
    "    # Authentication\n",
    "    client = bigquery.Client.from_service_account_json('./nwo-sample-5f8915fdc5ec.json')\n",
    "\n",
    "    #Perform a query.\n",
    "    QUERY = (\n",
    "        'SELECT * FROM `nwo-sample.graph.reddit` '\n",
    "        'WHERE LOWER(body) LIKE \"%{}%\" '\n",
    "        'ORDER BY created_utc DESC '\n",
    "        'LIMIT {}').format(query_term.lower(), limit)\n",
    "\n",
    "    query_job = client.query(QUERY)  # API request\n",
    "\n",
    "    return query_job.to_dataframe()\n",
    "\n",
    "\n",
    "df_reddit = make_query_reddit(\"Gamestop\", 500)\n",
    "\n",
    "print(df_reddit.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Twitter Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 20)\n"
     ]
    }
   ],
   "source": [
    "def make_query_twitter(query_term, limit):\n",
    "    \"\"\"Function to query the twitter dataset for a particular \n",
    "    term and return the data as a pandas dataframe\"\"\"\n",
    "    \n",
    "    # Authentication\n",
    "    client = bigquery.Client.from_service_account_json('./nwo-sample-5f8915fdc5ec.json')\n",
    "\n",
    "    #Perform a query.\n",
    "    QUERY = (\n",
    "        'SELECT * FROM `nwo-sample.graph.tweets` '\n",
    "        'WHERE LOWER(tweet) LIKE \"%{}%\" '\n",
    "        'ORDER BY created_at DESC '\n",
    "        'LIMIT {}').format(query_term.lower(), limit)\n",
    "\n",
    "    query_job = client.query(QUERY)  # API request\n",
    "\n",
    "    return query_job.to_dataframe()\n",
    "\n",
    "df_twitter = make_query_twitter(\"Gamestop\", 500)\n",
    "\n",
    "print(df_twitter.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Backing up datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 8) (500, 20)\n"
     ]
    }
   ],
   "source": [
    "df_reddit.to_pickle('./gamestop_reddit.pkl')\n",
    "df_twitter.to_pickle('./gamestop_twitter.pkl')\n",
    "\n",
    "df_reddit = pd.read_pickle('./gamestop_reddit.pkl')\n",
    "df_twitter = pd.read_pickle('./gamestop_twitter.pkl')\n",
    "\n",
    "print(df_reddit.shape, df_twitter.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Cleaning up datasets\n",
    "\n",
    "- Reddit Dataset\n",
    " - Removing URLs from body \n",
    " - Combining \"subreddit\" with \"body\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 1)\n"
     ]
    }
   ],
   "source": [
    "def clean_data_reddit(df_reddit):\n",
    "    \"\"\"Function to clean the reddit dataset by removing URLs, \n",
    "    instances of \".com\", and combining the \"subreddit\" and \"body\" \n",
    "    columns into a \"text\"\n",
    "    The function returns a smaller dataset with only the text column\"\"\"\n",
    "    \n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    df_reddit['body'] = df_reddit.body.apply(lambda x: url_pattern.sub(r'', x))\n",
    "\n",
    "    df_reddit['body'] = df_reddit.body.apply(lambda x: x.replace('.com', ''))\n",
    "\n",
    "    df_reddit['text'] = df_reddit.subreddit.fillna('') + ' ' + df_reddit.body.fillna('')\n",
    "\n",
    "    df_reddit = df_reddit[['text']]\n",
    "    \n",
    "    return df_reddit\n",
    "\n",
    "df_reddit = clean_data_reddit(df_reddit)\n",
    "\n",
    "print(df_reddit.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Twitter Dataset\n",
    " - Pulling out hashtags from lists\n",
    " - Removing URLs from tweets \n",
    " - Combining \"hashtags\" with \"tweet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 1)\n"
     ]
    }
   ],
   "source": [
    "def clean_data_twitter(df_twitter):\n",
    "    \"\"\"Function to clean the twitter dataset by removing URLs, \n",
    "    instances of \".com\", and combining the \"hashtags\" and \"tweet\" \n",
    "    columns into a \"text\"\n",
    "    The function returns a smaller dataset with only the text column\"\"\"\n",
    "\n",
    "    df_twitter['hashtags'] = df_twitter.hashtags.apply(lambda x: ' '.join(x))\n",
    "\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    df_twitter['tweet'] = df_twitter.tweet.apply(lambda x: url_pattern.sub(r'', x))\n",
    "\n",
    "    df_twitter['tweet'] = df_twitter.tweet.apply(lambda x: x.replace('.com', ''))\n",
    "\n",
    "    df_twitter['text'] = df_twitter.hashtags.fillna('') + ' ' + df_twitter.tweet.fillna('')\n",
    "\n",
    "    df_twitter = df_twitter[['text']]\n",
    "    \n",
    "    return df_twitter\n",
    "    \n",
    "df_twitter = clean_data_twitter(df_twitter)\n",
    "\n",
    "print(df_twitter.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Merging the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([df_reddit, df_twitter], ignore_index=True)\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Backing up datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "df.to_pickle('./gamestop_combined.pkl')\n",
    "\n",
    "df = pd.read_pickle('./gamestop_combined.pkl')\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Cleanup\n",
    "\n",
    "- Deleting individual datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_reddit, df_twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Co occurence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>00 10k</th>\n",
       "      <th>00 bought</th>\n",
       "      <th>00 links</th>\n",
       "      <th>00 pm</th>\n",
       "      <th>000</th>\n",
       "      <th>000 acquiring</th>\n",
       "      <th>000 cook</th>\n",
       "      <th>000 fermetures</th>\n",
       "      <th>000 finally</th>\n",
       "      <th>...</th>\n",
       "      <th>zero knowledge</th>\n",
       "      <th>zero location</th>\n",
       "      <th>zero priors</th>\n",
       "      <th>zkorcjtc3u</th>\n",
       "      <th>zoomed</th>\n",
       "      <th>zoomed let</th>\n",
       "      <th>zoomer</th>\n",
       "      <th>zoomer cohen</th>\n",
       "      <th>zslp</th>\n",
       "      <th>zslp pic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00 10k</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00 bought</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00 links</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00 pm</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22980 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           00  00 10k  00 bought  00 links  00 pm  000  000 acquiring  \\\n",
       "00          0       1          1         1      1    0              0   \n",
       "00 10k      1       0          0         0      0    0              0   \n",
       "00 bought   1       0          0         0      0    0              0   \n",
       "00 links    1       0          0         0      0    0              0   \n",
       "00 pm       1       0          0         0      0    0              0   \n",
       "\n",
       "           000 cook  000 fermetures  000 finally  ...  zero knowledge  \\\n",
       "00                0               0            0  ...               0   \n",
       "00 10k            0               0            0  ...               0   \n",
       "00 bought         0               0            0  ...               0   \n",
       "00 links          0               0            0  ...               0   \n",
       "00 pm             0               0            0  ...               0   \n",
       "\n",
       "           zero location  zero priors  zkorcjtc3u  zoomed  zoomed let  zoomer  \\\n",
       "00                     0            0           0       0           0       0   \n",
       "00 10k                 0            0           0       0           0       0   \n",
       "00 bought              0            0           0       0           0       0   \n",
       "00 links               0            0           0       0           0       0   \n",
       "00 pm                  0            0           0       0           0       0   \n",
       "\n",
       "           zoomer cohen  zslp  zslp pic  \n",
       "00                    0     0         0  \n",
       "00 10k                0     0         0  \n",
       "00 bought             0     0         0  \n",
       "00 links              0     0         0  \n",
       "00 pm                 0     0         0  \n",
       "\n",
       "[5 rows x 22980 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_co_occurence(df_text_column):    \n",
    "    \"\"\"This function takes the text column of a dataset as an input and generates a co-occurence matrix, \n",
    "    which is then returned as a pandas dataframe to preserve feature names\"\"\"\n",
    "    \n",
    "    count_vectorizer = CountVectorizer(stop_words = 'english', ngram_range=(1, 2), lowercase=True)\n",
    "    vectorized_matrix = count_vectorizer.fit_transform(df_text_column)\n",
    "\n",
    "    co_occurrence_matrix = (vectorized_matrix.T * vectorized_matrix)\n",
    "    co_occurrence_matrix.setdiag(0)\n",
    "    \n",
    "    return pd.DataFrame(co_occurrence_matrix.A, \n",
    "                        columns=count_vectorizer.get_feature_names(),\n",
    "                        index=count_vectorizer.get_feature_names())\n",
    "\n",
    "df_com = generate_co_occurence(df['text']) \n",
    "\n",
    "df_com.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph(df_com):\n",
    "    \"\"\"This function takes a pandas dataframe representation of a co-occurence matrix, \n",
    "    and converts it into a graph where the word co-occurence is represented as nodes and edges\"\"\"\n",
    "\n",
    "    df_com.values[np.tril(np.ones(df_com.shape)).astype(np.bool)] = 0\n",
    "    \n",
    "    df_com_stacked = df_com.stack()\n",
    "\n",
    "    df_com_stacked = df_com_stacked[df_com_stacked >= 1].rename_axis(('source', 'target')).reset_index(name='weight')\n",
    "    \n",
    "    graph = nx.from_pandas_edgelist(df_com_stacked,  edge_attr=True)\n",
    "    \n",
    "    return graph\n",
    "\n",
    "G = generate_graph(df_com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Most closely associated trends "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedSet(['wallstreetbets', 'stocks', 'stock', 'just', 'games', 'short', 'buy', 'gme', 'shares', 'company'])\n"
     ]
    }
   ],
   "source": [
    "def find_neighbors(graph, query_term, topn):\n",
    "    \"\"\"This function takes a graph and a query term as input, and finds the nodes \n",
    "    associated with the query term of interest, and returns an ordered list of \n",
    "    it's neighboring nodes (trends). \n",
    "    The list is sorted based on the edge weight (descending) which comes from the Co-Occurence Matrix.\n",
    "    A second parameter for sorting is the uniqueness of the association based on how many \n",
    "    other nodes the neighbor is connected to (descending)\"\"\"\n",
    "    \n",
    "    node_of_interest = query_term.lower()\n",
    "    \n",
    "    neighbors = []\n",
    "\n",
    "    for neighbor in graph.neighbors(node_of_interest):\n",
    "        neighbors.append((neighbor, graph.get_edge_data(node_of_interest, neighbor)['weight'], len(graph.edges(neighbor))))\n",
    "\n",
    "    neighbors.sort(key=lambda x:(-x[1], x[2]))\n",
    "\n",
    "    return OrderedSet([neighbor[0] for neighbor in neighbors[:topn]])\n",
    "\n",
    "print(find_neighbors(G, 'Gamestop', 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
